{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HateSpeechDetector_Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiRhbiiQFsjq"
      },
      "source": [
        "# This program builds and trains a Transformer model for detecting hate speech in Twitter posts. \n",
        "\n",
        "This model was created using datasets we were able to find online at Kaggle. Unfortunately that set is too small for training an entire transformer from scratch. Instead, this model uses transfer learning with a pretrained BERT model followed by trainable transformer and dense layers.\n",
        "\n",
        "CITATIONS\n",
        "\n",
        "BERT Developed by:\n",
        "- Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova: \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\", 2018. [https://arxiv.org/abs/1810.04805]\n",
        "\n",
        "BERT model available from Tensorflow Hub at *https://tfhub.dev/google/collections/bert/1*\n",
        "\n",
        "Datasets:\n",
        "- Kaggle dataset uploaded by \"Rahul Agarwal\" at *https://www.kaggle.com/vkrahul/twitter-hate-speech?select=train_E6oV3lV.csv*\n",
        "\n",
        "- Our ML Team's custom hand-labelled dataset using the Twitter API\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXkVkCNKr_aX"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrxSFbb4k2aN",
        "outputId": "b2b5c0ec-53c5-471a-ac07-953a7e88e7ac"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Tensorflow\n",
        "import tensorflow as tf\n",
        "!pip install -q tensorflow-text\n",
        "!pip install --upgrade tensorflow_hub\n",
        "import tensorflow_text as tf_text\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_hub in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_hub) (3.17.3)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_hub) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow_hub) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoohiJL8r-23",
        "outputId": "3b7f968e-7f6a-4655-dbe8-2086bc55f115"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/MyDrive/Colab_Notebooks/models/ReRight')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ44ojm0VnZ-"
      },
      "source": [
        "Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LZpDjeu4Vnid",
        "outputId": "e0c0c8c4-bba7-49e7-f881-78ff107badf5"
      },
      "source": [
        "CHECKPOINT_PATH = 'HateSpeechDetector/checkpoint'\n",
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/Colab_Notebooks/models/ReRight'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dI5COahrsBCw"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Pe62RcNKIc5l",
        "outputId": "32d51608-e315-4ef6-e232-f22167789759"
      },
      "source": [
        "df_tweets = pd.read_csv('datasets/hatespeech.csv')\n",
        "df_tweets.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>It should be legal for a husband to rape his wife</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>dalits are lowlife scum</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>dalits are lowlives</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>It was a better world when women dare not ques...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>I don't work this hard so that those immigrant...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                                               text  label\n",
              "0           0  It should be legal for a husband to rape his wife      1\n",
              "1           1                            dalits are lowlife scum      1\n",
              "2           2                                dalits are lowlives      1\n",
              "3           3  It was a better world when women dare not ques...      1\n",
              "4           4  I don't work this hard so that those immigrant...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZevBv84nUT7"
      },
      "source": [
        "Clean up text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9bEM8oYnUae"
      },
      "source": [
        "def standardize_df(dataframe):\n",
        "    df = dataframe.copy()\n",
        "\n",
        "    # lowercase and split on spaces\n",
        "    df['text'] =  df['text'].map(lambda x: x.lower())\n",
        "\n",
        "    # remove irregular chars\n",
        "    def splitter(x):\n",
        "        return re.sub(pattern='([^a-zA-Z0-9])', repl=' ', string=x)\n",
        "\n",
        "    df['text'] =  df['text'].map(splitter)\n",
        "\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RcAMwxpnfRL"
      },
      "source": [
        "df_tweets = standardize_df(df_tweets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ASLCX-hu_gN"
      },
      "source": [
        "\"\"\"\n",
        "# tools for splitting by sentence length\n",
        "def split_on_wordnum(sentence, wordnum=5):\n",
        "    # split\n",
        "    sentence = sentence.split()\n",
        "    first_half = sentence[:wordnum]\n",
        "    second_half = sentence[wordnum:]\n",
        "    \n",
        "    # rejoin\n",
        "    first_half = ' '.join(first_half)\n",
        "    second_half = ' '.join(second_half)\n",
        "    return first_half, second_half\n",
        "\n",
        "def split_df_sentences(df, wordnum=5):\n",
        "    split_array = df['text'].map(lambda x: split_on_wordnum(x, wordnum)).to_list()\n",
        "    sentence_halves_df = pd.DataFrame.from_records(split_array, columns=['first_half', 'second_half'])\n",
        "    df2 = pd.concat([df,  sentence_halves_df], axis='columns' )\n",
        "    return df2\n",
        "\"\"\"\n",
        "\n",
        "def min_word_count(df):\n",
        "\n",
        "    df1 = df[df['text'].map(lambda x: len(x.split())) < 50]\n",
        "    df2 = df[df['text'].map(lambda x: len(x.split())) >= 50]\n",
        "    df3 = df[df['text'].map(lambda x: len(x.split())) >= 100]\n",
        "\n",
        "    return df1, df2, df3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Et2BWdoauuek"
      },
      "source": [
        "df1, df2, df3 = min_word_count(df_tweets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rJPSFxrYMoa"
      },
      "source": [
        "Test / train split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KizmtaamYOop"
      },
      "source": [
        "split = int(.1*len(df_tweets))\n",
        "df_tweets = df_tweets.sample(frac=1)\n",
        "\n",
        "df_test = df_tweets[:split]\n",
        "df_valid = df_tweets[split: 2*split]\n",
        "df_train = df_tweets[2*split:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxrvQLwIgxcJ"
      },
      "source": [
        "Class weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2auPTG3gycS",
        "outputId": "a93d38d4-a8dd-4e49-c940-ec1fdb83a0b3"
      },
      "source": [
        "# compute class weightings \n",
        "# (used balance data during training)\n",
        "classes = df_train['label'].unique()\n",
        "num_samples = tf.cast(len(df_train), tf.float32)\n",
        "\n",
        "CLASS_WEIGHTS = {}\n",
        "print('Class Balances:' )\n",
        "for i in classes:\n",
        "    prop = len(df_train[df_train['label'] == i]) / num_samples\n",
        "    CLASS_WEIGHTS[i] = 1. / (2 * prop)\n",
        "    print(f'Class {i}: {100*prop:.1f}%, weighting = {CLASS_WEIGHTS[i]:.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class Balances:\n",
            "Class 1: 33.4%, weighting = 1.497\n",
            "Class 0: 66.6%, weighting = 0.751\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kKKLFGPgzcE"
      },
      "source": [
        "TF Dataset conversion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skZuGoyXTJVp"
      },
      "source": [
        "def convert_to_dataset(df):\n",
        "\n",
        "    dataset_x = tf.data.Dataset.from_tensor_slices(df[['text']])\n",
        "    dataset_y = tf.data.Dataset.from_tensor_slices(df[['label']])\n",
        "    dataset = tf.data.Dataset.zip((dataset_x, dataset_y))\n",
        "    return dataset\n",
        "\n",
        "dataset_test = convert_to_dataset(df_test)\n",
        "dataset_valid = convert_to_dataset(df_valid)\n",
        "dataset_train = convert_to_dataset(df_train)\n",
        "\n",
        "# evaluation sets\n",
        "df1, df2, df3 = min_word_count(df_tweets)\n",
        "ds_under50_all = convert_to_dataset(df1)\n",
        "ds_over50_all = convert_to_dataset(df2)\n",
        "ds_over100_all = convert_to_dataset(df3)\n",
        "\n",
        "df1, df2, df3 = min_word_count(df_tweets[:2*split])\n",
        "ds_under50_validation = convert_to_dataset(df1)\n",
        "ds_over50_validation = convert_to_dataset(df2)\n",
        "ds_over100_validation = convert_to_dataset(df3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dY9w4ULT1n1T",
        "outputId": "9b03d020-87bf-4535-ac36-18642d70b7ca"
      },
      "source": [
        "len(ds_over50_validation)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "579"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RC8Rwqt4XS4L"
      },
      "source": [
        "Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7w61bgnXT1Y"
      },
      "source": [
        "def hatespeech_detector(num_heads=1, key_dim=128):\n",
        "    #layers\n",
        "    preprocessor = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
        "    encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\",\n",
        "                              trainable=False)\n",
        "    mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
        "\n",
        "    # inputs\n",
        "    text = tf.keras.layers.Input(shape=(), dtype=tf.string)\n",
        "    inputs= [text]\n",
        "\n",
        "    # locked transfer model\n",
        "    text = preprocessor(text)\n",
        "    text = encoder(text)\n",
        "    text = text[\"sequence_output\"]\n",
        "    \n",
        "    # fine tuning head\n",
        "    text = mha(key=text, query=text, value=text)\n",
        "    text = tf.keras.layers.Reshape([-1])(text)\n",
        "    pred = tf.keras.layers.Dense(1, activation='sigmoid')(text)\n",
        "    \n",
        "    outputs = [pred]\n",
        "    return tf.keras.Model(inputs, outputs, name='hatespeech_detector')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2lW3EQpWzt4"
      },
      "source": [
        "Build"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8YQY9ifTJgx"
      },
      "source": [
        "hatespeech_detector_model = hatespeech_detector(num_heads=1, key_dim=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62nW3tWIaOVd",
        "outputId": "71cb27ec-f1db-4102-ed77-4b1bed7f0abe"
      },
      "source": [
        "hatespeech_detector_model.summary()\n",
        "print()\n",
        "hatespeech_detector_model(tf.constant(['test sentence']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"hatespeech_detector\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None,)]            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer (KerasLayer)        {'input_word_ids': ( 0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer_1 (KerasLayer)      {'encoder_outputs':  109482241   keras_layer[0][0]                \n",
            "                                                                 keras_layer[0][1]                \n",
            "                                                                 keras_layer[0][2]                \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention (MultiHead (None, 128, 768)     394368      keras_layer_1[0][14]             \n",
            "                                                                 keras_layer_1[0][14]             \n",
            "                                                                 keras_layer_1[0][14]             \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 98304)        0           multi_head_attention[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            98305       reshape[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 109,974,914\n",
            "Trainable params: 492,673\n",
            "Non-trainable params: 109,482,241\n",
            "__________________________________________________________________________________________________\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.5028259]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtGM_0xL0V2F"
      },
      "source": [
        "Load weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_5ZWQ4N0V9k",
        "outputId": "d5caf1f4-5ca5-4a7a-ebef-fc1577497a5d"
      },
      "source": [
        "hatespeech_detector_model.load_weights(CHECKPOINT_PATH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fdeefda4bd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nexSOlDWWyee"
      },
      "source": [
        "Compile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZXOv8LNTJjd"
      },
      "source": [
        "hatespeech_detector_model.compile(optimizer=tf.keras.optimizers.Adam(0.0001),\n",
        "                                  loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0),  # can experiment with label smoothing values\n",
        "                                  metrics=['binary_accuracy', \n",
        "                                           tf.keras.metrics.AUC(num_thresholds=200, curve='ROC', name='ROC'),\n",
        "                                           tf.keras.metrics.AUC(num_thresholds=200, curve='PR', name='Precision-Recall')],\n",
        "                                  steps_per_execution=4)\n",
        "                                  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yz7QRcj5hIeN"
      },
      "source": [
        "Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7mclxeShIoB"
      },
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=8)\n",
        "\n",
        "# checkpoint to save progress during training\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=CHECKPOINT_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1_yJOaSQAMV"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCZqekSHWv8a"
      },
      "source": [
        "EPOCHS = 1\n",
        "STEPS_PER_EPOCH = 10\n",
        "\n",
        "hist = hatespeech_detector_model.fit(x=dataset_train.batch(512, drop_remainder=True).shuffle(int(10e6)), \n",
        "                                     validation_data=dataset_valid.batch(512, drop_remainder=True).shuffle(int(10e6)), \n",
        "                                     epochs=EPOCHS,\n",
        "                                     steps_per_epoch=STEPS_PER_EPOCH,\n",
        "                                     class_weight=CLASS_WEIGHTS,\n",
        "                                     callbacks=[early_stopping, model_checkpoint],\n",
        "                                     validation_steps=5\n",
        "                                     )\n",
        "\n",
        "\"\"\"\n",
        "hatespeech_detector_model.saved_history = hist\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z48pcw6DXGeG"
      },
      "source": [
        "Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d4hIUHiXHRk"
      },
      "source": [
        "\"\"\" \n",
        "This code is from the TF tutorial at \n",
        "https://www.tensorflow.org/tutorials/structured_data/imbalanced_data, \n",
        "with only minor modifications \n",
        "\"\"\"\n",
        "\n",
        "mpl.rcParams['figure.figsize'] = (12, 10)\n",
        "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
        "\n",
        "def plot_metrics(history):\n",
        "  \n",
        "  metrics = list(history.history.keys())[:4]  # update this with our chosen metrics\n",
        "  for n, metric in enumerate(metrics):\n",
        "    name = metric.replace(\"_\",\" \").capitalize()\n",
        "    plt.subplot(2,2,n+1)\n",
        "    plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')\n",
        "    plt.plot(history.epoch, history.history['val_'+metric],\n",
        "             color=colors[0], linestyle=\"--\", label='Val')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel(name)\n",
        "\n",
        "    plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU27OfVXXNq8"
      },
      "source": [
        "plot_metrics(history=hist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5zJy4lIUH9q"
      },
      "source": [
        "Evaluatation experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsH6wROYWHve"
      },
      "source": [
        "hatespeech_detector_model.evaluate(ds_under50_validation.batch(128))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra0Zg2SgUFUu"
      },
      "source": [
        "hatespeech_detector_model.evaluate(ds_over50_validation.batch(128))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF9HI0YRUVG1"
      },
      "source": [
        "hatespeech_detector_model.evaluate(ds_over100_validation.batch(128))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAMz-wEWWKLF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}