{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ClusterBios",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hioARJbgkprB"
      },
      "source": [
        "## Clustering user by Twitter Bio\n",
        "\n",
        "This program uses our custom built Twitter bio datasets for the purpose of matching users into similarity groups. \n",
        "\n",
        "**TODO: It is important to create/find a better text preprocessing code before drawing conclusions from the data. Twitter bios are highly irregular in format and style. Standard preprocessing techniques alone do not seem well suited for the job.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcJKeecrLUxV"
      },
      "source": [
        "**Citations**\n",
        "\n",
        "Twitter text preprocessing:\n",
        "- \"Basic Tweet Preprocessing Method With Python\" by \n",
        "Anil Emrah, [https://medium.com/analytics-vidhya/basic-tweet-preprocessing-method-with-python-56b4e53854a1]\n",
        "\n",
        "Text Encoding: LEGAL BERT Model series by:\n",
        "\n",
        "- I. Chalkidis, M. Fergadiotis, P. Malakasiotis, N. Aletras and I. Androutsopoulos. \n",
        "\"LEGAL-BERT: The Muppets straight out of Law School\". \n",
        "In Findings of Empirical Methods in Natural Language Processing (EMNLP 2020) \n",
        "(Short Papers), to be held online, 2020. (https://aclanthology.org/2020.findings-emnlp.261)\n",
        "\n",
        "Pretrained Model Repo / Implementation:\n",
        "\n",
        "- https://huggingface.co/nlpaueb/legal-bert-base-uncased"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx3eXn-_qfrM"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cF2wQJOpuVL1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31b5d014-0b5b-4c8b-f515-1e747e759311"
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.mixture import BayesianGaussianMixture\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "\n",
        "# Pretrained Transformers from HuggingFace\n",
        "!pip install transformers\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# general\n",
        "import os\n",
        "import string\n",
        "import re\n",
        "\n",
        "# text preprocessing\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNrZAuyC2iR7",
        "outputId": "2db78217-0ada-4cc3-dc52-94eb698f6a62"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DATASETS_FOLDER = '/content/drive/MyDrive/Colab_Notebooks/models/ReRight/datasets'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcpdNC7nUF2l"
      },
      "source": [
        "# Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwLZKEOIIaOz"
      },
      "source": [
        "# Load datasets from excel files\n",
        "\n",
        "def load_datasets(dataset_folder=os.path.join(DATASETS_FOLDER,'Twitter_Bios')):\n",
        "\n",
        "    df = None\n",
        "    for dirpath, dirnames, filenames in os.walk(dataset_folder):\n",
        "        for file in filenames:\n",
        "            if df is None:\n",
        "                df = pd.read_excel(file)\n",
        "            else:\n",
        "                df = pd.concat([df, pd.read_excel(file)])\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "df = load_datasets()   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "4bDyj89CK7fx",
        "outputId": "b08a765a-c6c4-4b81-f616-8355717c06d2"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>username</th>\n",
              "      <th>description</th>\n",
              "      <th>name</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>EmJaRo2</td>\n",
              "      <td>#IStandWithRosieDuffield</td>\n",
              "      <td>Emma Robertson</td>\n",
              "      <td>1217216916986699776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NotACommunist24</td>\n",
              "      <td>20. Autistic. Bi. Based af. â¤RESIDENT EVIL!â¤ I...</td>\n",
              "      <td>Based Syndicalist</td>\n",
              "      <td>1309627886941409280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>BarbaraRowley7</td>\n",
              "      <td>she/her. Former Healthcare Worker. Science ner...</td>\n",
              "      <td>Barbara Rowley</td>\n",
              "      <td>1170232400569192448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>dreamchxild</td>\n",
              "      <td>ğ˜§ğ˜¦ğ˜¦ğ˜­ğ˜ªğ˜¯ğ˜¨ ğ˜®ğ˜ºğ˜´ğ˜¦ğ˜­ğ˜§ ğ˜­ğ˜ªğ˜¬ğ˜¦ ğ˜ªğ˜® ğ˜¯ğ˜°ğ˜³ğ˜®ğ˜¢ ğ˜«ğ˜¦ğ˜¢ğ˜¯ğ˜¦. ğ˜¯ğ˜ªğ˜¨ğ˜©ğ˜µ ğ˜´ğ˜¤ğ˜³ğ˜ª...</td>\n",
              "      <td>ğƒğˆğ€à¼„ *.ï¾Ÿâ™¡</td>\n",
              "      <td>727246415232159744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>rockinfabblue</td>\n",
              "      <td>Full of Myself.ğŸ’–ğŸ¤—\\nVirgo sunâ™\\nGemini risingâ™Š\\...</td>\n",
              "      <td>Tiffany</td>\n",
              "      <td>1473144504</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          username  ...                   id\n",
              "0          EmJaRo2  ...  1217216916986699776\n",
              "1  NotACommunist24  ...  1309627886941409280\n",
              "2   BarbaraRowley7  ...  1170232400569192448\n",
              "3      dreamchxild  ...   727246415232159744\n",
              "4    rockinfabblue  ...           1473144504\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rCRYlmyMQCi"
      },
      "source": [
        "Preprocessing\n",
        "\n",
        "*TODO: improve text prep. We believe it should be possible to find/create steps more tailored to Twitter data.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77Ey99xSNC-6"
      },
      "source": [
        "# NOTE:  This code is due to the post \"Basic Tweet Preprocessing Method With Python\" by Anil Emrah\n",
        "# and hosted on Github at https://gist.github.com/anilemrah/a390f0f7008670e6187ef980ee10c1da#file-preprocess_tweet-py\n",
        "def preprocess_tweet(text):\n",
        "    \"\"\"\n",
        "    Function comes from https://medium.com/analytics-vidhya/basic-tweet-preprocessing-method-with-python-56b4e53854a1\n",
        "    \"\"\"\n",
        "    # Check characters to see if they are in punctuation\n",
        "    nopunc = [char for char in text if char not in string.punctuation]\n",
        "\n",
        "    # Join the characters again to form the string.\n",
        "    nopunc = ''.join(nopunc)\n",
        "\n",
        "    # convert text to lower-case\n",
        "    nopunc = nopunc.lower()\n",
        "\n",
        "    # remove URLs\n",
        "    nopunc = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', '', nopunc)\n",
        "    nopunc = re.sub(r'http\\S+', '', nopunc)\n",
        "\n",
        "    # remove usernames\n",
        "    nopunc = re.sub('@[^\\s]+', '', nopunc)\n",
        "\n",
        "    # remove the # in #hashtag\n",
        "    nopunc = re.sub(r'#([^\\s]+)', r'\\1', nopunc)\n",
        "\n",
        "    # remove repeated characters\n",
        "    nopunc = word_tokenize(nopunc)\n",
        "\n",
        "    # remove stopwords from final word list\n",
        "    return [word for word in nopunc if word not in stopwords.words('english')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5O7eiawMPaS"
      },
      "source": [
        "df['description'] = df['description'].apply(preprocess_tweet)\n",
        "df['name'] = df['name'].apply(preprocess_tweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "RPXFt4TgMmux",
        "outputId": "e2ba21dc-0914-440b-caea-30074b4f4b6c"
      },
      "source": [
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>username</th>\n",
              "      <th>description</th>\n",
              "      <th>name</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>EmJaRo2</td>\n",
              "      <td>[istandwithrosieduffield]</td>\n",
              "      <td>[emmarobertson]</td>\n",
              "      <td>1217216916986699776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NotACommunist24</td>\n",
              "      <td>[20autisticbibasedafâ¤residentevilâ¤illtweetingc...</td>\n",
              "      <td>[basedsyndicalist]</td>\n",
              "      <td>1309627886941409280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>BarbaraRowley7</td>\n",
              "      <td>[sheherformerhealthcareworkersciencenerdwashha...</td>\n",
              "      <td>[barbararowley]</td>\n",
              "      <td>1170232400569192448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>dreamchxild</td>\n",
              "      <td>[ğ˜§ğ˜¦ğ˜¦ğ˜­ğ˜ªğ˜¯ğ˜¨ğ˜®ğ˜ºğ˜´ğ˜¦ğ˜­ğ˜§ğ˜­ğ˜ªğ˜¬ğ˜¦ğ˜ªğ˜®ğ˜¯ğ˜°ğ˜³ğ˜®ğ˜¢ğ˜«ğ˜¦ğ˜¢ğ˜¯ğ˜¦ğ˜¯ğ˜ªğ˜¨ğ˜©ğ˜µğ˜´ğ˜¤ğ˜³ğ˜ªğ˜£ğ˜£ğ˜­ğ˜¦ğ˜³ğ˜®ğ˜¢...</td>\n",
              "      <td>[ğƒğˆğ€à¼„ï¾Ÿâ™¡]</td>\n",
              "      <td>727246415232159744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>rockinfabblue</td>\n",
              "      <td>[fullmyselfğŸ’–ğŸ¤—virgosunâ™geminirisingâ™Šsagittarius...</td>\n",
              "      <td>[tiffany]</td>\n",
              "      <td>1473144504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>ZMafereka</td>\n",
              "      <td>[siyaqhuba]</td>\n",
              "      <td>[pandemicpapi]</td>\n",
              "      <td>1347793743345307649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>judithpark13</td>\n",
              "      <td>[mummy2preciousgirlsğŸ’–ğŸ’–]</td>\n",
              "      <td>[judithpark]</td>\n",
              "      <td>382876047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>taycolmenero</td>\n",
              "      <td>[wildthings]</td>\n",
              "      <td>[taylorcolmenero]</td>\n",
              "      <td>305766960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>norlenemm</td>\n",
              "      <td>[zimbabweanfounderherwombbfollowreproductivehe...</td>\n",
              "      <td>[norlenem]</td>\n",
              "      <td>1251104064793952258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Lilies09</td>\n",
              "      <td>[lifesworthdamntillshout]</td>\n",
              "      <td>[englishrose]</td>\n",
              "      <td>54625997</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          username  ...                   id\n",
              "0          EmJaRo2  ...  1217216916986699776\n",
              "1  NotACommunist24  ...  1309627886941409280\n",
              "2   BarbaraRowley7  ...  1170232400569192448\n",
              "3      dreamchxild  ...   727246415232159744\n",
              "4    rockinfabblue  ...           1473144504\n",
              "5        ZMafereka  ...  1347793743345307649\n",
              "6     judithpark13  ...            382876047\n",
              "7     taycolmenero  ...            305766960\n",
              "8        norlenemm  ...  1251104064793952258\n",
              "9         Lilies09  ...             54625997\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QT0AlWDqk3u"
      },
      "source": [
        "## Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49fz38MyQ1cP"
      },
      "source": [
        "Tokenizer\n",
        "\n",
        "*note: this was successfully used in our rights violations code, but we recommend finding/creating a preprocessors better suited to Twitter data*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gm9DalQ_koR8"
      },
      "source": [
        "\"\"\"\n",
        "Tokenizer created by I. Chalkidis, M. Fergadiotis, P. Malakasiotis, N. Aletras and I. Androutsopoulos. \n",
        "\"LEGAL-BERT: The Muppets straight out of Law School\". In Findings of Empirical Methods in Natural Language Processing (EMNLP 2020) \n",
        "(Short Papers), to be held online, 2020. (https://aclanthology.org/2020.findings-emnlp.261)\n",
        "\n",
        "PRETRAINED MODEL IMPLEMENTATION from https://huggingface.co/nlpaueb/legal-bert-base-uncased\n",
        "\"\"\"\n",
        "\n",
        "# Tokenizer\n",
        "# This is a specialized tokenizer designed for use on the dataset\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/bert-base-uncased-echr\")\n",
        "\n",
        "# ECHR Dataset\n",
        "# apply text preprocessor\n",
        "texts = dataframe_human_rights['facts'].to_list()\n",
        "tokens_echr = tokenizer(texts,\n",
        "                   padding=True,\n",
        "                   truncation=True,\n",
        "                   max_length=256,  # pad/truncate to uniform size\n",
        "                   return_tensors=\"pt\")  # return in PyTorch format\n",
        "\n",
        "masked_tokens_echr = tokens_echr['input_ids'] * tokens_echr['attention_mask']                   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ep_MFwLXQ273"
      },
      "source": [
        "Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqeKEIpRrkuP"
      },
      "source": [
        "# Count Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# convert to string for fitting vectorizer\n",
        "corpus = range(torch.max(tokens_echr['input_ids']))\n",
        "corpus = [str(c) for c in corpus]  \n",
        "\n",
        "# fit\n",
        "tfidf_vectorizer.fit(corpus)\n",
        "\n",
        "# convert to string for applying vectorizer\n",
        "masked_tokens_list = masked_tokens_echr.tolist()\n",
        "masked_tokens_echr_string = [' '.join([str(num) for num in masked_tokens_list[i]]) \n",
        "                                for i in range(len(masked_tokens_list))]\n",
        "\n",
        "encoded_data = tfidf_vectorizer.transform(masked_tokens_echr_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3ubOttSRC8c",
        "outputId": "2e8a065f-10c5-4dbd-ab09-bc2537690c77"
      },
      "source": [
        "encoded_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1000x29980 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 132098 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wgtzfqVUAiI"
      },
      "source": [
        "# Mixture Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYPz3qC3DXuh"
      },
      "source": [
        "Dimension Reduction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxiAZoTYDZoW"
      },
      "source": [
        "pca_transform = PCA(n_components=10)\n",
        "\n",
        "# if using TF-IDF vectorized data\n",
        "encoded_data_condensed = pca_transform.fit_transform(encoded_data.todense())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WehEwvib2Rnx"
      },
      "source": [
        "Cluster Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bixDHrB2Q8l"
      },
      "source": [
        "mixture_model = BayesianGaussianMixture(n_components=5, random_state=142)\n",
        "mixture_model.fit(encoded_data_condensed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9r0XnXoLxs7H"
      },
      "source": [
        "clusters_assignments = mixture_model.predict(encoded_data_condensed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiEuOYmOYmr6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbdb419a-43be-4a3f-b878-415402190c1e"
      },
      "source": [
        "clusters_probs = mixture_model.predict_proba(encoded_data_condensed)\n",
        "clusters_probs[:5,:]\n",
        "print(clusters_probs.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoUXy4LBFhGx"
      },
      "source": [
        "Combine into dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDEdHg-gFldm"
      },
      "source": [
        "# hard assignments\n",
        "dataframe_human_rights['clusters_assignments'] = clusters_assignments\n",
        "dataframe_human_rights.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5jMi_jQGMB0",
        "outputId": "bfab4657-0104-4252-e2ff-aa41c3c7ecf0"
      },
      "source": [
        "# soft assignments\n",
        "soft_assignments_df = pd.DataFrame(clusters_probs, columns=['clusters_probs_0', 'clusters_probs_1', 'clusters_probs_2', 'clusters_probs_3', 'clusters_probs_4'])\n",
        "soft_assignments_df.head(3)\n",
        "print(len(soft_assignments_df))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnBxOS-dGdJI",
        "outputId": "f63f6d97-05df-407e-c53d-ceaea8600eeb"
      },
      "source": [
        "df = pd.concat([dataframe_human_rights, soft_assignments_df.reindex(dataframe_human_rights.index)], axis='columns', join='inner')\n",
        "len(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jnFKoAgJRse"
      },
      "source": [
        "## Interpretations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6CqeiX3nGOG"
      },
      "source": [
        "TODO:\n",
        "- Interpret Results (what do the clusters represent?"
      ]
    }
  ]
}